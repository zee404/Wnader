{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Java JDK\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#Downloading Spark\n",
    "!wget -q http://apache.mirrors.pair.com/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz \n",
    "#Unzipping the hadoop file\n",
    "!tar -xvf spark-3.0.1-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### SPARK SETUP ################################\n",
    "#Install findspark\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Spark session using findspark lib\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the path of the files\n",
    "reviews_file ='data/hotels/details/reviews_clean.csv'\n",
    "hotels_file = 'data/hotels/details/hotel_info_clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readFiles(filename):\n",
    "    data = spark.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').\\\n",
    "                load(filename,header=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Read the data files\n",
    "reviews = readFiles(reviews_file)\n",
    "hotels = readFiles(hotels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reviews\n",
    "#print the schema now and check that timestamp column is dropped\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into train and test where 80% data is in train and remaining is test\n",
    "train, test = data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a recommendation model using Alternating Least Squares method\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "model = ALS(userCol=\"user_id\", itemCol=\"hotel_id\", ratingCol=\"rating\", nonnegative=True, coldStartStrategy=\"drop\", maxIter=10).fit(train)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and print the RMSE of the ALS model\n",
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"New RMSE: \", evaluator.evaluate(model.transform(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing ALS with Cross Validation\n",
    "    \n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to improve the performance of the original model using cross validation and solve the cold-start problem.\n",
    "# we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "model = ALS(userCol=\"user_id\", itemCol=\"hotel_id\", ratingCol=\"rating\", nonnegative=True, coldStartStrategy=\"drop\", maxIter=10)\n",
    "\n",
    "#For Parameter tuning of the ALS model we use ParamGridBuilder function\n",
    "#We tune two parameters \n",
    "#1. The Regularization parameter ranging from 0.1, 0.01, 0.001, 0.0001\n",
    "#2. The rank for matrix factorization\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.regParam, [0.1, 0.05, 0.01, 0.001]) \\\n",
    "    .addGrid(model.rank, [5, 10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "#Defining a cross-validator object\n",
    "#Setting up CV and adding parameters. We will be performing a 5 fold CV\n",
    "crossvalidation = CrossValidator(estimator = model,\n",
    "                     estimatorParamMaps = paramGrid,\n",
    "                     evaluator = evaluator,\n",
    "                     numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "Best_model = crossvalidation.fit(train).bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Best_model\n",
    "print(type(Best_model))\n",
    "#Complete the code below to extract the ALS model parameters\n",
    "print(\"**Best Model**\")\n",
    "#Rank\n",
    "print(\"Rank: \", Best_model._java_obj.parent().getRank())\n",
    "#MaxIter\n",
    "print(\"MaxIter: \", Best_model._java_obj.parent().getMaxIter())\n",
    "#RegParam\n",
    "print(\"RegParam: \", Best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the RMSE on test data using the best set of parameters obtained after cross validation\n",
    "print(\"Best RMSE value is: \", evaluator.evaluate(Best_model.transform(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pred = Best_model.transform(test)\n",
    "# pred.show(10)\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "preds = Best_model.recommendForAllUsers(5)\n",
    "preds.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = preds.select(preds.user_id, explode_outer(preds.recommendations).alias(\"recommendation\"))\n",
    "final_preds.show(10)\n",
    "hotel_recommendation = final_preds.toPandas()\n",
    "hotel_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hotel_recommendation[['hotel_id', 'rating']] = hotel_recommendation['recommendation'].apply(lambda x: pd.Series([int(x[0]), x[1]]))\n",
    "hotel_recommendation = hotel_recommendation.drop(\"recommendation\", axis=1)\n",
    "hotel_recommendation['hotel_id'] = hotel_recommendation['hotel_id'].astype(int)\n",
    "hotel_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_items = Best_model.recommendForAllItems(5)\n",
    "preds_items.show()\n",
    "\n",
    "Best_model.save(\"weight/als_model_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = readFiles(hotels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.join(hotels, pred[\"hotel_id\"] ==  hotels[\"id\"]).select(\"user_id\",\"hotel_name\",\"hotel_rating\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_an_user = pred.where(pred.user_id==1088).join(hotels, pred[\"hotel_id\"] ==  hotels[\"id\"]).select(\"user_id\",\"hotel_name\",\"hotel_rating\",\"prediction\")\n",
    "for_an_user.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
